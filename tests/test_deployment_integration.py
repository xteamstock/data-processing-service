#!/usr/bin/env python3
"""
End-to-end integration test for the reimplemented data-processing service.

This script tests the dual-output jobs functionality (GCS + BigQuery + Media Events)
against deployed Cloud Run services.

Features tested:
1. Health checks for both services
2. Data ingestion service crawl trigger
3. Data processing service dual-output jobs:
   - Job 1: GCS processed data upload
   - Job 2: BigQuery analytics insertion
   - Job 3: Media detection and event publishing
4. Event flow validation
5. Data integrity verification

Requirements:
- Both services deployed to Cloud Run
- Pub/Sub topics and push subscriptions configured
- Service URLs accessible
- GOOGLE_CLOUD_PROJECT environment variable set

Usage:
    export GOOGLE_CLOUD_PROJECT=social-analytics-prod
    export DATA_INGESTION_URL=https://data-ingestion-service-url
    export DATA_PROCESSING_URL=https://data-processing-service-url
    python test_deployment_integration.py
"""

import os
import sys
import json
import time
import requests
import logging
import subprocess
from datetime import datetime
from typing import Dict, Any, Optional, List
from google.cloud import storage, bigquery, pubsub_v1
import google.auth

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataProcessingIntegrationTester:
    """Test the reimplemented data-processing service with dual-output jobs."""
    
    def __init__(self):
        """Initialize the tester with service URLs and GCP clients."""
        self.project_id = os.getenv('GOOGLE_CLOUD_PROJECT')
        self.data_ingestion_url = os.getenv('DATA_INGESTION_URL')
        self.data_processing_url = os.getenv('DATA_PROCESSING_URL')
        
        if not self.project_id:
            raise ValueError("GOOGLE_CLOUD_PROJECT environment variable required")
        if not self.data_ingestion_url:
            raise ValueError("DATA_INGESTION_URL environment variable required")
        if not self.data_processing_url:
            raise ValueError("DATA_PROCESSING_URL environment variable required")
            
        # Initialize GCP clients
        self.storage_client = storage.Client()
        self.bigquery_client = bigquery.Client()
        self.pubsub_client = pubsub_v1.SubscriberClient()
        
        # Set up authentication
        self._setup_authentication()
        
        # Test configuration
        self.raw_data_bucket = 'social-analytics-raw-data'
        self.processed_data_bucket = 'social-analytics-processed-data'
        self.bigquery_dataset = 'social_analytics'
        self.bigquery_table = 'posts'
        
        logger.info(f"Project ID: {self.project_id}")
        logger.info(f"Data Ingestion URL: {self.data_ingestion_url}")
        logger.info(f"Data Processing URL: {self.data_processing_url}")
        logger.info(f"Authentication: {self.auth_method}")
        logger.info(f"Raw Data Bucket: {self.raw_data_bucket}")
        logger.info(f"Processed Data Bucket: {self.processed_data_bucket}")
        logger.info(f"BigQuery Table: {self.project_id}.{self.bigquery_dataset}.{self.bigquery_table}")
    
    def _setup_authentication(self):
        """Set up authentication for Cloud Run services."""
        try:
            result = subprocess.run(
                ['gcloud', 'auth', 'print-identity-token'],
                capture_output=True,
                text=True,
                check=True
            )
            token = result.stdout.strip()
            
            self.auth_headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json'
            }
            self.auth_method = "gcloud identity token"
            logger.info("‚úÖ Using gcloud identity token for Cloud Run authentication")
            
        except Exception as gcloud_error:
            logger.warning(f"gcloud identity token failed: {str(gcloud_error)}")
            
            try:
                from google.oauth2 import id_token
                from google.auth.transport.requests import Request
                
                audience = self.data_ingestion_url
                credentials, _ = google.auth.default()
                token = id_token.fetch_id_token(Request(), audience)
                
                self.auth_headers = {
                    'Authorization': f'Bearer {token}',
                    'Content-Type': 'application/json'
                }
                self.auth_method = "ADC identity token"
                logger.info("‚úÖ Using ADC identity token for Cloud Run authentication")
                
            except Exception as adc_error:
                logger.error(f"ADC identity token failed: {str(adc_error)}")
                raise ValueError("Authentication failed. Please run 'gcloud auth login'")
    
    def _make_authenticated_request(self, method: str, url: str, **kwargs) -> requests.Response:
        """Make an authenticated request to Cloud Run service."""
        if 'headers' not in kwargs:
            kwargs['headers'] = {}
        kwargs['headers'].update(self.auth_headers)
        return requests.request(method, url, **kwargs)
    
    def test_health_checks(self) -> bool:
        """Test health endpoints for both services."""
        logger.info("=== Testing Health Checks ===")
        
        # Test data ingestion health
        try:
            response = self._make_authenticated_request('GET', f"{self.data_ingestion_url}/health", timeout=10)
            if response.status_code == 200:
                health_data = response.json()
                logger.info(f"‚úÖ Data Ingestion Service: Health check passed")
                logger.info(f"   Service: {health_data.get('service')}")
                logger.info(f"   Version: {health_data.get('version')}")
                logger.info(f"   Environment: {health_data.get('environment')}")
            else:
                logger.error(f"‚ùå Data Ingestion Service: Health check failed ({response.status_code})")
                return False
        except Exception as e:
            logger.error(f"‚ùå Data Ingestion Service: Health check error: {str(e)}")
            return False
        
        # Test data processing health
        try:
            response = self._make_authenticated_request('GET', f"{self.data_processing_url}/health", timeout=10)
            if response.status_code == 200:
                health_data = response.json()
                logger.info(f"‚úÖ Data Processing Service: Health check passed")
                logger.info(f"   Service: {health_data.get('service')}")
                logger.info(f"   Version: {health_data.get('version')}")
                logger.info(f"   Environment: {health_data.get('environment')}")
            else:
                logger.error(f"‚ùå Data Processing Service: Health check failed ({response.status_code})")
                return False
        except Exception as e:
            logger.error(f"‚ùå Data Processing Service: Health check error: {str(e)}")
            return False
        
        return True
    
    def test_data_processing_endpoint(self) -> bool:
        """Test data processing service test endpoint."""
        logger.info("=== Testing Data Processing Service Test Endpoint ===")
        
        test_data = {
            "test_message": "Dual-output jobs integration test",
            "timestamp": datetime.utcnow().isoformat(),
            "test_features": ["gcs_upload", "bigquery_insert", "media_detection"]
        }
        
        try:
            response = self._make_authenticated_request(
                'POST',
                f"{self.data_processing_url}/api/v1/test",
                json=test_data,
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                logger.info("‚úÖ Data Processing Service: Test endpoint working")
                logger.info(f"   Message: {result.get('message')}")
                logger.info(f"   Received data: {result.get('received_data', {}).get('test_message')}")
                return True
            else:
                logger.error(f"‚ùå Data Processing Service: Test endpoint failed ({response.status_code})")
                logger.error(f"Response: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Data Processing Service: Test endpoint error: {str(e)}")
            return False
    
    def trigger_test_crawl(self) -> Optional[str]:
        """Trigger a test crawl using data ingestion service."""
        logger.info("=== Triggering Test Crawl for Dual-Output Jobs ===")
        
        # Use production-quality test parameters with media content
        crawl_params = {
            "dataset_id": "gd_lkaxegm826bjpoo9m5",
            "platform": "facebook", 
            "competitor": "nutifood",
            "brand": "growplus-nutifood",
            "category": "sua-bot-tre-em",
            "url": "https://www.facebook.com/GrowPLUScuaNutiFood/?locale=vi_VN",
            "start_date": "2024-12-24",
            "end_date": "2024-12-24",
            "include_profile_data": True,
            "num_of_posts": 3  # Reduced for faster testing but enough for media detection
        }
        
        logger.info(f"Test parameters: {json.dumps(crawl_params, indent=2)}")
        
        try:
            response = self._make_authenticated_request(
                'POST',
                f"{self.data_ingestion_url}/api/v1/crawl/trigger",
                json=crawl_params,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                crawl_id = result.get('crawl_id')
                
                if crawl_id:
                    logger.info(f"‚úÖ Crawl triggered successfully: {crawl_id}")
                    logger.info(f"   Snapshot ID: {result.get('snapshot_id')}")
                    logger.info(f"   Background processing: {result.get('background_processing', {}).get('enabled', False)}")
                    return crawl_id
                else:
                    logger.error("‚ùå Crawl trigger response missing crawl_id")
                    return None
            else:
                logger.error(f"‚ùå Crawl trigger failed ({response.status_code})")
                logger.error(f"Response: {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Crawl trigger error: {str(e)}")
            return None
    
    def monitor_crawl_completion(self, crawl_id: str, max_wait_minutes: int = 20) -> Dict[str, Any]:
        """Monitor crawl status until completion or timeout."""
        logger.info(f"=== Monitoring Crawl Completion: {crawl_id} ===")
        
        start_time = time.time()
        max_wait_seconds = max_wait_minutes * 60
        poll_interval = 30
        
        while (time.time() - start_time) < max_wait_seconds:
            try:
                response = self._make_authenticated_request(
                    'GET',
                    f"{self.data_ingestion_url}/api/v1/crawl/{crawl_id}/status",
                    timeout=10
                )
                
                if response.status_code == 200:
                    status_data = response.json()
                    current_status = status_data.get('status')
                    ready_for_download = status_data.get('ready_for_download', False)
                    background_active = status_data.get('background_processing', {}).get('active', False)
                    
                    elapsed_minutes = (time.time() - start_time) / 60
                    logger.info(f"Status after {elapsed_minutes:.1f}m: {current_status} (ready: {ready_for_download}, bg: {background_active})")
                    
                    # Check completion status
                    if ready_for_download and not background_active:
                        logger.info(f"‚úÖ Crawl {crawl_id} completed and should have triggered data processing!")
                        return status_data
                    elif current_status == 'error':
                        logger.error(f"‚ùå Crawl {crawl_id} failed: {status_data.get('error_message')}")
                        return status_data
                    
                    time.sleep(poll_interval)
                    
                else:
                    logger.error(f"‚ùå Status check failed ({response.status_code}): {response.text}")
                    time.sleep(poll_interval)
                    
            except Exception as e:
                logger.error(f"‚ùå Status check error: {str(e)}")
                time.sleep(poll_interval)
        
        logger.warning(f"‚è∞ Monitoring timeout after {max_wait_minutes} minutes")
        
        try:
            response = self._make_authenticated_request('GET', f"{self.data_ingestion_url}/api/v1/crawl/{crawl_id}/status")
            if response.status_code == 200:
                return response.json()
        except:
            pass
            
        return {'status': 'timeout', 'crawl_id': crawl_id}
    
    def verify_gcs_processed_data(self, crawl_id: str, metadata: Dict[str, Any]) -> bool:
        """Verify that processed data was uploaded to GCS."""
        logger.info("=== Verifying GCS Processed Data Upload (Job 1) ===")
        
        try:
            # Construct expected path pattern
            platform = metadata.get('platform', 'facebook')
            competitor = metadata.get('competitor', 'nutifood')
            brand = metadata.get('brand', 'growplus-nutifood')
            category = metadata.get('category', 'sua-bot-tre-em')
            
            # Search for files with crawl_id in processed data bucket
            bucket = self.storage_client.bucket(self.processed_data_bucket)
            
            # Look for files in the hierarchical structure containing our crawl_id
            prefix = f"raw_data/platform={platform}/competitor={competitor}/brand={brand}/category={category}/"
            
            logger.info(f"Searching for processed files in: gs://{self.processed_data_bucket}/{prefix}")
            logger.info(f"Looking for files containing crawl_id: {crawl_id}")
            
            blobs = list(bucket.list_blobs(prefix=prefix))
            
            # Find files with our crawl_id
            matching_files = []
            for blob in blobs:
                if crawl_id in blob.name:
                    matching_files.append(blob)
            
            if matching_files:
                logger.info(f"‚úÖ Found {len(matching_files)} processed data files:")
                for blob in matching_files:
                    logger.info(f"   üìÅ gs://{self.processed_data_bucket}/{blob.name} ({blob.size} bytes)")
                    
                    # Verify file content is valid JSON
                    try:
                        content = blob.download_as_text()
                        lines = content.strip().split('\n')
                        for i, line in enumerate(lines[:3]):  # Check first 3 lines
                            json.loads(line)  # Verify valid JSON
                        logger.info(f"      ‚úÖ Valid JSONL content ({len(lines)} records)")
                    except Exception as e:
                        logger.warning(f"      ‚ö†Ô∏è File content validation warning: {str(e)}")
                
                return True
            else:
                logger.error(f"‚ùå No processed data files found for crawl_id: {crawl_id}")
                
                # List all recent files for debugging
                recent_blobs = [blob for blob in blobs if (datetime.utcnow() - blob.time_created).total_seconds() < 3600]
                if recent_blobs:
                    logger.info(f"Recent files in bucket (last hour):")
                    for blob in recent_blobs[:5]:
                        logger.info(f"   üìÅ {blob.name} (created: {blob.time_created})")
                
                return False
                
        except Exception as e:
            logger.error(f"‚ùå GCS verification error: {str(e)}")
            return False
    
    def verify_bigquery_data(self, crawl_id: str) -> bool:
        """Verify that data was inserted into BigQuery."""
        logger.info("=== Verifying BigQuery Analytics Data (Job 2) ===")
        
        try:
            # Query for posts with our crawl_id
            query = f"""
            SELECT 
                post_id,
                crawl_id,
                snapshot_id,
                platform,
                competitor,
                brand,
                category,
                post_content,
                date_posted,
                engagement_metrics,
                media_metadata,
                processing_metadata
            FROM `{self.project_id}.{self.bigquery_dataset}.{self.bigquery_table}`
            WHERE crawl_id = @crawl_id
            ORDER BY processed_date DESC
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("crawl_id", "STRING", crawl_id)
                ]
            )
            
            logger.info(f"Querying BigQuery for crawl_id: {crawl_id}")
            
            query_job = self.bigquery_client.query(query, job_config=job_config)
            results = list(query_job.result())
            
            if results:
                logger.info(f"‚úÖ Found {len(results)} posts in BigQuery:")
                for i, row in enumerate(results):
                    logger.info(f"   üìä Post {i+1}: {row.post_id}")
                    logger.info(f"       Platform: {row.platform}")
                    logger.info(f"       Competitor: {row.competitor}")
                    logger.info(f"       Brand: {row.brand}")
                    logger.info(f"       Date Posted: {row.date_posted}")
                    logger.info(f"       Content Length: {len(row.post_content or '')}")
                    
                    # Check engagement metrics
                    if row.engagement_metrics:
                        likes = row.engagement_metrics.get('likes', 0)
                        comments = row.engagement_metrics.get('comments', 0)
                        shares = row.engagement_metrics.get('shares', 0)
                        logger.info(f"       Engagement: {likes} likes, {comments} comments, {shares} shares")
                    
                    # Check media metadata
                    if row.media_metadata:
                        media_count = row.media_metadata.get('media_count', 0)
                        has_video = row.media_metadata.get('has_video', False)
                        has_image = row.media_metadata.get('has_image', False)
                        logger.info(f"       Media: {media_count} attachments (video: {has_video}, image: {has_image})")
                    
                    # Check processing metadata
                    if row.processing_metadata:
                        schema_version = row.processing_metadata.get('schema_version', 'unknown')
                        quality_score = row.processing_metadata.get('data_quality_score', 0.0)
                        logger.info(f"       Processing: schema v{schema_version}, quality {quality_score:.2f}")
                
                return True
            else:
                logger.error(f"‚ùå No posts found in BigQuery for crawl_id: {crawl_id}")
                
                # Check if table exists and has recent data
                recent_query = f"""
                SELECT crawl_id, COUNT(*) as post_count, MAX(processed_date) as latest_processed
                FROM `{self.project_id}.{self.bigquery_dataset}.{self.bigquery_table}`
                WHERE processed_date >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
                GROUP BY crawl_id
                ORDER BY latest_processed DESC
                LIMIT 5
                """
                
                try:
                    recent_job = self.bigquery_client.query(recent_query)
                    recent_results = list(recent_job.result())
                    
                    if recent_results:
                        logger.info("Recent crawls in BigQuery (last hour):")
                        for row in recent_results:
                            logger.info(f"   üìä {row.crawl_id}: {row.post_count} posts (latest: {row.latest_processed})")
                    else:
                        logger.warning("No recent data found in BigQuery table")
                except Exception as e:
                    logger.warning(f"Could not check recent data: {str(e)}")
                
                return False
                
        except Exception as e:
            logger.error(f"‚ùå BigQuery verification error: {str(e)}")
            return False
    
    def check_media_events(self, crawl_id: str) -> bool:
        """Check if media processing events were published (Job 3)."""
        logger.info("=== Checking Media Processing Events (Job 3) ===")
        
        # Note: This is a simplified check since we can't easily subscribe to Pub/Sub in this test
        # In a real scenario, you'd have a subscription to monitor these events
        
        logger.info("üì° Media event verification:")
        logger.info("   This test verifies media detection occurred during BigQuery processing")
        logger.info("   Media processing events would be published to Pub/Sub topic:")
        logger.info(f"   Topic: social-analytics-media-processing-requested")
        
        # Check if BigQuery data contains media metadata indicating media detection worked
        try:
            query = f"""
            SELECT 
                post_id,
                media_metadata.media_count,
                media_metadata.has_video,
                media_metadata.has_image,
                media_metadata.media_processing_requested
            FROM `{self.project_id}.{self.bigquery_dataset}.{self.bigquery_table}`
            WHERE crawl_id = @crawl_id
            AND media_metadata.media_count > 0
            """
            
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("crawl_id", "STRING", crawl_id)
                ]
            )
            
            query_job = self.bigquery_client.query(query, job_config=job_config)
            media_results = list(query_job.result())
            
            if media_results:
                logger.info(f"‚úÖ Found {len(media_results)} posts with media attachments:")
                total_media = 0
                videos_detected = 0
                images_detected = 0
                
                for row in media_results:
                    media_count = row[1] or 0
                    has_video = row[2] or False
                    has_image = row[3] or False
                    processing_requested = row[4] or False
                    
                    total_media += media_count
                    if has_video:
                        videos_detected += 1
                    if has_image:
                        images_detected += 1
                    
                    logger.info(f"   üé¨ Post {row[0]}: {media_count} media (video: {has_video}, image: {has_image}, requested: {processing_requested})")
                
                logger.info(f"   üìä Total: {total_media} media files detected")
                logger.info(f"   üìπ Videos: {videos_detected} posts")
                logger.info(f"   üñºÔ∏è Images: {images_detected} posts")
                logger.info(f"‚úÖ Media detection working correctly - events would be published")
                
                return True
            else:
                logger.warning("‚ö†Ô∏è No posts with media found - this may be expected if test data has no media")
                logger.info("   Media detection logic was still executed during processing")
                return True  # Not an error if test data doesn't have media
                
        except Exception as e:
            logger.error(f"‚ùå Media event verification error: {str(e)}")
            return False
    
    def run_full_integration_test(self) -> bool:
        """Run the complete dual-output jobs integration test."""
        logger.info("üöÄ Starting Data Processing Dual-Output Jobs Integration Test")
        logger.info("=" * 80)
        
        # Step 1: Health checks
        if not self.test_health_checks():
            logger.error("‚ùå Health checks failed - stopping test")
            return False
        
        # Step 2: Test data processing service
        if not self.test_data_processing_endpoint():
            logger.error("‚ùå Data processing test failed - stopping test")
            return False
        
        # Step 3: Trigger test crawl
        crawl_id = self.trigger_test_crawl()
        if not crawl_id:
            logger.error("‚ùå Failed to trigger test crawl - stopping test")
            return False
        
        # Step 4: Monitor crawl completion
        final_status = self.monitor_crawl_completion(crawl_id, max_wait_minutes=20)
        
        if final_status.get('status') not in ['ready', 'completed']:
            logger.error(f"‚ùå Crawl did not complete successfully: {final_status.get('status')}")
            return False
        
        # Step 5: Verify dual-output jobs
        logger.info("üîç Verifying Dual-Output Jobs Results...")
        
        # Extract metadata for verification
        crawl_metadata = {
            'platform': 'facebook',
            'competitor': 'nutifood',
            'brand': 'growplus-nutifood',
            'category': 'sua-bot-tre-em'
        }
        
        # Allow some time for processing to complete
        logger.info("‚è≥ Waiting 60 seconds for processing to complete...")
        time.sleep(60)
        
        # Verify Job 1: GCS Processed Data Upload
        gcs_success = self.verify_gcs_processed_data(crawl_id, crawl_metadata)
        
        # Verify Job 2: BigQuery Analytics Insertion
        bigquery_success = self.verify_bigquery_data(crawl_id)
        
        # Verify Job 3: Media Detection and Event Publishing
        media_success = self.check_media_events(crawl_id)
        
        # Final results
        logger.info("=" * 80)
        logger.info("üìä DUAL-OUTPUT JOBS TEST RESULTS:")
        logger.info(f"   Job 1 (GCS Upload): {'‚úÖ PASS' if gcs_success else '‚ùå FAIL'}")
        logger.info(f"   Job 2 (BigQuery Insert): {'‚úÖ PASS' if bigquery_success else '‚ùå FAIL'}")
        logger.info(f"   Job 3 (Media Events): {'‚úÖ PASS' if media_success else '‚ùå FAIL'}")
        
        overall_success = gcs_success and bigquery_success and media_success
        
        if overall_success:
            logger.info("üéâ Integration test PASSED!")
            logger.info("All dual-output jobs are working correctly.")
            logger.info(f"Test crawl_id: {crawl_id}")
        else:
            logger.error("üí• Integration test FAILED!")
            logger.error("One or more dual-output jobs failed.")
            logger.error("Check the logs above for detailed error information.")
        
        return overall_success

def main():
    """Main entry point for the integration test."""
    try:
        tester = DataProcessingIntegrationTester()
        success = tester.run_full_integration_test()
        
        sys.exit(0 if success else 1)
        
    except Exception as e:
        logger.error(f"üí• Integration test crashed: {str(e)}")
        sys.exit(1)

if __name__ == '__main__':
    main()